#!/usr/bin/env python
import sys
import threading
import time
from typing import Tuple

from math import pi
import cv2 as cv
import numpy as np
from scipy.spatial.transform import Rotation
import rospy
from geometry_msgs.msg import PoseStamped
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
from cv_bridge import CvBridge, CvBridgeError

# import some common detectron2 utilities
from detectron2.engine import DefaultPredictor
from detectron2.utils.logger import setup_logger
from detectron2.utils.visualizer import Visualizer
from detectron2_ros.msg import Result
from sensor_msgs.msg import Image, RegionOfInterest
from sklearn.decomposition import PCA


class Detectron2node(object):
    def __init__(self):
        rospy.logwarn("Initializing")
        setup_logger()

        self._bridge = CvBridge()
        self._last_msg = None
        self._last_depth_msg = None
        self._msg_lock = threading.Lock()
        self._depth_msg_lock = threading.Lock()
        self._image_counter = 0

        self.cfg = get_cfg()
        self.cfg.merge_from_file(self.load_param("~config"))
        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = self.load_param(
            "~detection_threshold"
        )  # set threshold for this model
        self.cfg.MODEL.WEIGHTS = self.load_param("~model")
        self.predictor = DefaultPredictor(self.cfg)
        self._class_names = MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]).get(
            "thing_classes", None
        )

        self._visualization = self.load_param("~visualization", True)
        self._result_pub = rospy.Publisher("~result", Result, queue_size=1)
        self._vis_pub = rospy.Publisher("~visualization", Image, queue_size=1)
        self._grasp_pub = rospy.Publisher("~grasp", PoseStamped, queue_size=1)
        self._sub = rospy.Subscriber(
            self.load_param("~input"), Image, self.callback_image, queue_size=1
        )
        self._depth_sub = rospy.Subscriber(
            "/camera/aligned_depth_to_color/image_meters",
            Image,
            self.callback_depth_image,
            queue_size=1,
        )
        self.start_time = time.time()
        rospy.logwarn("Initialized")

    def run(self):
        rate = rospy.Rate(100)
        while not rospy.is_shutdown():
            if self._msg_lock.acquire(False) and self._depth_msg_lock.acquire(False):
                img_msg = self._last_msg
                self._last_msg = None
                depth_msg = self._last_depth_msg
                self._last_depth_msg = None
                self._msg_lock.release()
                self._depth_msg_lock.release()

            else:
                rate.sleep()
                continue

            if img_msg and depth_msg:
                self._image_counter = self._image_counter + 1
                if (self._image_counter % 11) == 10:
                    rospy.loginfo(
                        "Images detected per second=%.2f",
                        float(self._image_counter) / (time.time() - self.start_time),
                    )

                np_image = self.convert_to_cv_image(img_msg)

                outputs = self.predictor(np_image)
                # print(outputs)
                result = outputs["instances"].to("cpu")
                result_msg, sa_points_all = self.getResult(result)

                self.publish_grasp(sa_points_all, depth_msg)
                self._result_pub.publish(result_msg)

                # Visualize results
                if self._visualization:
                    scale = 1.2
                    v = Visualizer(
                        np_image[:, :, ::-1],
                        MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]),
                        scale=scale,
                    )

                    # temp output
                    _temp_output = outputs["instances"].to("cpu")
                    # _class_ids = _temp_output.pred_classes if _temp_output.has("pred_classes") else None
                    # _names = np.array(self._class_names)[_class_ids.numpy()]

                    # for kkk, _name in enumerate(_names):
                    #     if _name == 'toilet':
                    #         print(_temp_output.pred_classes[_class_ids.numpy()[kkk]])
                    #         _temp_output.pred_classes[_class_ids.numpy()[kkk]] = 'tofu'
                    #     if _name == 'person':
                    #         print(_temp_output.pred_classes[_class_ids.numpy()[kkk]])
                    #         _temp_output.pred_classes[_class_ids.numpy()[kkk]] = 'tofu'

                    v = v.draw_instance_predictions(_temp_output)
                    img = v.get_image()[:, :, ::-1]
                    img2 = np.ascontiguousarray(img)
                    # add grasp
                    if len(sa_points_all) != 0:
                        for sa_points in sa_points_all:
                            cv.line(
                                img2,
                                (int(sa_points[1] * scale), int(sa_points[0] * scale)),
                                (int(sa_points[3] * scale), int(sa_points[2] * scale)),
                                (0, 0, 255),
                                5,
                            )

                            cv.circle(
                                img2,
                                (int(sa_points[5] * scale), int(sa_points[4] * scale)),
                                10,
                                (0, 255, 255),
                                -1,
                            )

                    image_msg = self._bridge.cv2_to_imgmsg(img2)
                    self._vis_pub.publish(image_msg)

            rate.sleep()

    def smallest_axis(self, mask):
        mask_xy = np.argwhere(mask != 0)
        center = mask_xy.mean(axis=0)

        # run pca
        pca = PCA(n_components=2)
        pca.fit(mask_xy)
        # choose smallest component
        comp = pca.components_[1]
        var = pca.explained_variance_[1]
        comp = comp * var * 0.1
        x1 = center[0] - comp[0]
        y1 = center[1] - comp[1]

        x2 = comp[0] + center[0]
        y2 = comp[1] + center[1]

        # calcualte angle
        angle = np.arctan2(comp[1], comp[0]) + np.pi / 2
        if angle >= np.pi:
            angle = angle - np.pi
        elif angle <= -np.pi:
            angle = angle + np.pi

        return x1, y1, x2, y2, center[0], center[1], angle # center[0] = y, center[1] = x

    def getResult(self, predictions):

        boxes = predictions.pred_boxes if predictions.has("pred_boxes") else None

        if predictions.has("pred_masks"):
            masks = np.asarray(predictions.pred_masks)
        else:
            return

        result_msg = Result()
        result_msg.header = self._header
        result_msg.class_ids = (
            predictions.pred_classes if predictions.has("pred_classes") else None
        )
        result_msg.class_names = np.array(self._class_names)[
            result_msg.class_ids.numpy()
        ]

        # # replace toilet to tofu
        # _temp = np.array(self._class_names)[result_msg.class_ids.numpy()]
        # for i, nnn in enumerate(result_msg.class_names):
        #     if nnn == 'toilet':
        #         result_msg.class_names[i] = 'tofu'
        #     elif nnn == 'person':
        #         result_msg.class_names[i] = 'tofu'

        result_msg.scores = predictions.scores if predictions.has("scores") else None

        sa_points_all = list()

        for i, (x1, y1, x2, y2) in enumerate(boxes):
            mask = np.zeros(masks[i].shape, dtype="uint8")
            mask[masks[i, :, :]] = 255

            # do PCA
            # if result_msg.class_names[i] == 'tv':
            sa_points = self.smallest_axis(mask)
            print(result_msg.class_names[i], sa_points[-1] * 180 / 3.14)
            sa_points_all.append(sa_points)

            mask = self._bridge.cv2_to_imgmsg(mask)
            result_msg.masks.append(mask)

            box = RegionOfInterest()
            box.x_offset = np.uint32(x1)
            box.y_offset = np.uint32(y1)
            box.height = np.uint32(y2 - y1)
            box.width = np.uint32(x2 - x1)
            result_msg.boxes.append(box)

        return result_msg, sa_points_all

    def publish_grasp(self, sa_points_all, depth_msg) -> None:
        if not sa_points_all:
            return
        depth_img = Detectron2node.convert_to_depth_img(depth_msg)
        sa_points_all.sort(key=lambda t: Detectron2node.get_depth_at_point(depth_img, 
            (int(t[5]), int(t[4])))
        )
        sa_points = sa_points_all[0]

        object_center = (int(sa_points[5]), int(sa_points[4])) # x, y
        depth = Detectron2node.get_depth_at_point(depth_img, object_center)
        if depth == 0 or depth == np.nan:
            return
        x, y, z, r, _, _ =  Detectron2node.get_pose_at_img_coordinate(
            depth, object_center, sa_points[6]
        )
        if not x or not y:
            return
        if r > pi / 2: # make angle acute
            r = -(pi - r)
        elif r < -pi / 4:
            r = pi + r
        r += pi

        grasp_msg = PoseStamped()
        grasp_msg.header.stamp = rospy.Time.now()
        grasp_msg.header.frame_id = "camera_color_frame"

        grasp_msg.pose.position.x = x
        grasp_msg.pose.position.y = y
        grasp_msg.pose.position.z = z - 0.2
        ow, ox, oy, oz = quaternion_from_euler(r, 0, 0)
        grasp_msg.pose.orientation.w = ow
        grasp_msg.pose.orientation.x = ox
        grasp_msg.pose.orientation.y = oy
        grasp_msg.pose.orientation.z = oz
        self._grasp_pub.publish(grasp_msg)

    @staticmethod
    def convert_to_depth_img(depth_msg):
        return np.ndarray(
            shape=(depth_msg.height, depth_msg.width),
            dtype=np.float32,
            buffer=depth_msg.data,
        )

    @staticmethod
    def get_depth_at_point(depth_img, object_center) -> float:
        return depth_img[object_center[1]][object_center[0]]

    @staticmethod
    def get_pose_at_img_coordinate(depth, coord: Tuple[int, int], angle: float):
        fx = fy = 419.0289001464844 #TODO get from camera info
        cx, cy = 425.84820556640625 * 1.5, 235.1192169189453 * 1.5
        X = (coord[0] - cx) / fx * depth
        Y = (coord[1] - cy) / fy * depth
        return X/2, Y/2, depth, angle, 0, 0

    def convert_to_cv_image(self, image_msg):
        if image_msg is None:
            return None

        self._width = image_msg.width
        self._height = image_msg.height
        channels = int(len(image_msg.data) / (self._width * self._height))

        encoding = None
        if image_msg.encoding.lower() in ["rgb8", "bgr8"]:
            encoding = np.uint8
        elif image_msg.encoding.lower() == "mono8":
            encoding = np.uint8
        elif image_msg.encoding.lower() == "16uc1":
            encoding = np.uint16
            channels = 1
        elif image_msg.encoding.lower() == "32fc1":
            encoding = np.float32
            channels = 1

        cv_img = np.ndarray(
            shape=(image_msg.height, image_msg.width, channels),
            dtype=encoding,
            buffer=image_msg.data,
        )

        if image_msg.encoding.lower() == "mono8":
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2GRAY)
        elif image_msg.encoding.lower() == "16uc1":
            cv_img = cv_img.astype("int32")
        else:
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2BGR)

        return cv_img

    def callback_image(self, msg):
        rospy.logdebug("Get an image")
        if self._msg_lock.acquire(False):
            self._last_msg = msg
            self._header = msg.header
            self._msg_lock.release()

    def callback_depth_image(self, msg):
        rospy.logdebug("Get a depth image")
        if self._depth_msg_lock.acquire(False):
            self._last_depth_msg = msg
            self._depth_header = msg.header
            self._depth_msg_lock.release()

    @staticmethod
    def load_param(param, default=None):
        new_param = rospy.get_param(param, default)
        rospy.loginfo("[Detectron2] %s: %s", param, new_param)
        return new_param

def quaternion_from_euler(r, p, y):
    rot = Rotation.from_euler('xyz', [r, p, y])
    return rot.as_quat()


def main(argv):
    rospy.init_node("detectron2_ros")
    node = Detectron2node()
    node.run()


if __name__ == "__main__":
    main(sys.argv)
