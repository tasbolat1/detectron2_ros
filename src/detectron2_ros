#!/usr/bin/env python
from multiprocessing.sharedctypes import Value
from os import spawnlp
import sys
import threading
import time
from typing import Tuple
from math import pi

import cv2 as cv
import numpy as np
from scipy.spatial.transform import Rotation
from sklearn.decomposition import PCA
import rospy
from geometry_msgs.msg import PoseStamped
from sensor_msgs.msg import CameraInfo
from std_msgs.msg import Float32MultiArray
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
from cv_bridge import CvBridge, CvBridgeError

# import some common detectron2 utilities
from detectron2.engine import DefaultPredictor
from detectron2.utils.logger import setup_logger
from detectron2.utils.visualizer import Visualizer
from detectron2_ros.msg import Result
from sensor_msgs.msg import Image, RegionOfInterest

LEFT_X_OFFSET = 70

class Detectron2node(object):
    def __init__(self):
        rospy.logwarn("Initializing")
        setup_logger()

        self._bridge = CvBridge()
        self._last_depth_msg, self._last_depth_raw_msg = None, None
        self._depth_msg_lock = threading.Lock()
        self._depth_raw_msg_lock = threading.Lock()
        self._image_counter = 0
        camera_info_msg = rospy.wait_for_message(
            "/camera/color/camera_info", CameraInfo
        )
        self.cam_K = np.array(camera_info_msg.K).reshape((3, 3))

        self.cfg = get_cfg()  # TODO find a way to remove this
        self.cfg.merge_from_file(self.load_param("~config"))
        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = self.load_param(
            "~detection_threshold"
        )  # set threshold for this model
        self.cfg.MODEL.WEIGHTS = self.load_param("~model")

        self._visualization = self.load_param("~visualization", True)
        self._result_pub = rospy.Publisher("~result", Result, queue_size=1)
        self._vis_pub = rospy.Publisher("~visualization", Image, queue_size=1)
        self._grasp_pub = rospy.Publisher("~grasp", Float32MultiArray, queue_size=1)
        # self._sub = rospy.Subscriber(
        #     self.load_param("~input"), Image, self.callback_image, queue_size=1
        # )
        self._depth_sub = rospy.Subscriber(
            "/camera/aligned_depth_to_color/image_meters",
            Image,
            self.callback_depth_image,
            queue_size=1,
        )
        self._depth_raw_sub = rospy.Subscriber(
            "/camera/aligned_depth_to_color/image_raw",
            Image,
            self.callback_depth_image_raw,
            queue_size=1,
        )
        self.start_time = time.time()
        rospy.logwarn("Initialized")

    def run(self):
        rate = rospy.Rate(100)
        while not rospy.is_shutdown():
            if self._depth_msg_lock.acquire(False) and self._depth_raw_msg_lock.acquire(
                False
            ):
                depth_msg, depth_msg_raw = (
                    self._last_depth_msg,
                    self._last_depth_raw_msg,
                )
                self._last_depth_msg, self._last_depth_raw_msg = None, None
                self._depth_msg_lock.release()
                self._depth_raw_msg_lock.release()

            else:
                rate.sleep()
                continue

            if depth_msg and depth_msg_raw:
                self._image_counter = self._image_counter + 1
                if (self._image_counter % 11) == 10:
                    rospy.loginfo(
                        "Images detected per second=%.2f",
                        float(self._image_counter) / (time.time() - self.start_time),
                    )

                depth_img = self.convert_to_depth_img(depth_msg)
                depth_img = np.nan_to_num(depth_img, nan=np.inf)
                depth_img_raw = self.convert_to_depth_img(depth_msg_raw, raw=True)

                num_components, segmap = self.segmap_from_depthmap_2(depth_img_raw)
                sa_points_array = []
                for class_id in range(1, num_components):
                    sa_points = self.smallest_axis(segmap, class_id=class_id)
                    if sa_points:
                        sa_points_array.append(sa_points)

                self.publish_grasp(sa_points_array, depth_img)

                # Visualize results
                if self._visualization:
                    scale = 1
                    img = Detectron2node.colormap_segmap(segmap)
                    # img = np.moveaxis(img, 0, -1)

                    # v = Visualizer(
                    #     img[:, :, ::-1].T,
                    #     MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]),
                    #     scale=scale,
                    # )

                    # temp output
                    # _temp_output = outputs["instances"].to("cpu") # removed
                    # _class_ids = _temp_output.pred_classes if _temp_output.has("pred_classes") else None
                    # _names = np.array(self._class_names)[_class_ids.numpy()]

                    # for kkk, _name in enumerate(_names):
                    #     if _name == 'toilet':
                    #         print(_temp_output.pred_classes[_class_ids.numpy()[kkk]])
                    #         _temp_output.pred_classes[_class_ids.numpy()[kkk]] = 'tofu'
                    #     if _name == 'person':
                    #         print(_temp_output.pred_classes[_class_ids.numpy()[kkk]])
                    #         _temp_output.pred_classes[_class_ids.numpy()[kkk]] = 'tofu'

                    # v = v.draw_instance_predictions()
                    # img = v.get_image()[
                    #     :, :, ::-1
                    # ]
                    img_contiguous = np.ascontiguousarray(img)
                    # add grasp
                    for sa_points in sa_points_array:
                        cv.line(
                            img_contiguous,
                            (int(sa_points[1] * scale), int(sa_points[0] * scale)),
                            (int(sa_points[3] * scale), int(sa_points[2] * scale)),
                            (0, 0, 255),
                            5,
                        )

                        cv.circle(
                            img_contiguous,
                            (int(sa_points[5] * scale), int(sa_points[4] * scale)),
                            10,
                            (0, 255, 255),
                            -1,
                        )

                    image_msg = self._bridge.cv2_to_imgmsg(img_contiguous)
                    self._vis_pub.publish(image_msg)

            rate.sleep()

    def segmap_from_depthmap(self, depth_img):
        min_depth = np.min(depth_img)
        mask = depth_img < (min_depth + 0.02)  # +2cm
        print("frac ", np.argwhere(mask).shape[0] / (mask.shape[0] * mask.shape[1]))
        return mask.astype(int)  # convert bool to 0 and 1

    def segmap_from_depthmap_2(self, depth_img):
        blur = cv.GaussianBlur(depth_img, (7, 7), 0)
        blur = cv.blur(blur, (17, 17))
        # cut out top right corner which is noisy
        blur[:110, 675:] = 1000

        TABLE_DEPTH = 563
        ret, thres = cv.threshold(
            blur,
            TABLE_DEPTH,
            255,
            cv.THRESH_BINARY_INV,
        )
        # Marker labelling
        ret, markers = cv.connectedComponents(np.uint8(thres))
        print("num components", ret)
        return ret, np.uint8(markers)

    @staticmethod
    def draw_box_around_segmap(segmap):
        # it seems like the box is only for visualisation, so there is no need to find the tightest bounding box here
        coords = np.argwhere(segmap)
        min_y, min_x = np.min(np.argwhere(coords), axis=0)
        max_y, max_x = np.max(np.argwhere(coords), axis=0)
        return (min_x, min_y, max_x, max_y)

    @staticmethod
    def colormap_segmap(segmap):
        # map 0-numclasses to rgb
        segmap *= 25
        return cv.applyColorMap(segmap, cv.COLORMAP_JET)

    def smallest_axis(self, mask, class_id=1):
        mask_xy = np.argwhere(mask == class_id)
        if len(mask_xy) == 0:
            return
        center = mask_xy.mean(axis=0)

        # run pca
        try:
            pca = PCA(n_components=2)
            pca.fit(mask_xy)
        except ValueError as e:
            print(e)
            return None

        # choose smallest component
        comp = pca.components_[1]
        var = pca.explained_variance_[1]
        comp = comp * var * 0.1
        x1 = center[0] - comp[0]
        y1 = center[1] - comp[1]

        x2 = comp[0] + center[0]
        y2 = comp[1] + center[1]

        # calculate angle
        angle = np.arctan2(comp[1], comp[0]) + np.pi / 2
        if angle >= np.pi:
            angle = angle - np.pi
        elif angle <= -np.pi:
            angle = angle + np.pi

        return (
            x1,
            y1,
            x2,
            y2,
            center[0],
            center[1],
            angle,
        )  # center[0] = y, center[1] = x

    def publish_grasp(self, sa_points_array, depth_img) -> None:
        grasp_msg = Float32MultiArray()
        data = []
        for i, sa_points in enumerate(sa_points_array):
            object_center = (int(sa_points[5]), int(sa_points[4]))  # x, y
            depth = Detectron2node.get_depth_at_point(depth_img, object_center)
            if depth == 0 or depth == np.nan:
                continue
            x, y, z, r, _, _ = self.get_pose_at_img_coordinate(
                depth, object_center, sa_points[6]
            )
            if not x or not y:
                continue
            
            # make some corrections
            x -= 0.015
            y += 0.012
            z -= 0.24
            
            r = pi / 2 - r
            if r > pi / 2:  # make angle acute
                r = -(pi - r)
            elif r < -pi / 2:
                r = pi + r
            
            data.extend([x, y, z, r])
        grasp_msg.data = data
        self._grasp_pub.publish(grasp_msg)

    def convert_to_depth_img(self, depth_msg, raw=False):
        depth_img = np.ndarray(
            shape=(depth_msg.height, depth_msg.width),
            dtype=np.uint16 if raw else np.float32,
            buffer=depth_msg.data,
        )[:, LEFT_X_OFFSET: int(23 / 32 * depth_msg.width)]
        # cropping here to remove irrelevant sections
        return depth_img

    # def normalise_angle(self, depth_img, h, w):
    #     if not pre
    #     center = (h / 2, w / 2)

    @staticmethod
    def get_depth_at_point(depth_img, object_center) -> float:
        return depth_img[object_center[1]][object_center[0]]

    def get_pose_at_img_coordinate(self, depth, coord: Tuple[int, int], angle: float):
        fx = self.cam_K[0, 0]
        fy = self.cam_K[1, 1]
        cx, cy = self.cam_K[0, 2], self.cam_K[1, 2]
        X = (LEFT_X_OFFSET + coord[0] - cx) / fx * depth
        Y = (coord[1] - cy) / fy * depth
        # print(X, Y, depth)
        return X, Y, depth, angle, 0, 0

    def convert_to_cv_image(self, image_msg):
        if image_msg is None:
            return None

        self._width = image_msg.width
        self._height = image_msg.height
        channels = int(len(image_msg.data) / (self._width * self._height))

        encoding = None
        if image_msg.encoding.lower() in ["rgb8", "bgr8"]:
            encoding = np.uint8
        elif image_msg.encoding.lower() == "mono8":
            encoding = np.uint8
        elif image_msg.encoding.lower() == "16uc1":
            encoding = np.uint16
            channels = 1
        elif image_msg.encoding.lower() == "32fc1":
            encoding = np.float32
            channels = 1

        cv_img = np.ndarray(
            shape=(image_msg.height, image_msg.width, channels),
            dtype=encoding,
            buffer=image_msg.data,
        )

        if image_msg.encoding.lower() == "mono8":
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2GRAY)
        elif image_msg.encoding.lower() == "16uc1":
            cv_img = cv_img.astype("int32")
        else:
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2BGR)

        return cv_img

    def callback_depth_image(self, msg):
        rospy.logdebug("Get a depth image")
        if self._depth_msg_lock.acquire(False):
            self._last_depth_msg = msg
            self._header = msg.header
            self._depth_msg_lock.release()

    def callback_depth_image_raw(self, msg):
        rospy.logdebug("Get a depth raw image")
        if self._depth_raw_msg_lock.acquire(False):
            self._last_depth_raw_msg = msg
            self._depth_raw_msg_lock.release()

    @staticmethod
    def load_param(param, default=None):
        new_param = rospy.get_param(param, default)
        rospy.loginfo("[Detectron2] %s: %s", param, new_param)
        return new_param


def quaternion_from_euler(r, p, y):
    rot = Rotation.from_euler("xyz", [r, p, y])
    return rot.as_quat()


def main(argv):
    rospy.init_node("detectron2_ros")
    node = Detectron2node()
    node.run()


if __name__ == "__main__":
    main(sys.argv)
