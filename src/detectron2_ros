#!/usr/bin/env python
from multiprocessing.sharedctypes import Value
from os import spawnlp
import sys
import threading
import time
from typing import Tuple
from collections import deque

import torch
import cv2 as cv
import numpy as np
from sklearn.decomposition import PCA
import rospy
from geometry_msgs.msg import PoseStamped
from sensor_msgs.msg import CameraInfo
from std_msgs.msg import Float32MultiArray
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
from cv_bridge import CvBridge, CvBridgeError

# import some common detectron2 utilities
from detectron2.engine import DefaultPredictor
from detectron2.utils.logger import setup_logger
from detectron2.utils.visualizer import Visualizer
from detectron2_ros.msg import Result
from sensor_msgs.msg import Image, RegionOfInterest

LEFT_X_OFFSET = 70


class Detectron2node(object):
    def __init__(self):
        rospy.logwarn("Initializing")
        setup_logger()

        self._bridge = CvBridge()
        self.object_detector = torch.hub.load("ultralytics/yolov5", "yolov5l")
        self.object_detector.conf = 0.01
        self._last_depth_msg, self._last_depth_raw_msg, self._last_image_msg = (
            None,
            None,
            None,
        )
        self._depth_msg_lock, self._depth_raw_msg_lock, self._image_msg_lock = (
            threading.Lock(),
            threading.Lock(),
            threading.Lock(),
        )
        self._image_counter = 0
        camera_info_msg = rospy.wait_for_message(
            "/camera/color/camera_info", CameraInfo
        )
        self.cam_K = np.array(camera_info_msg.K).reshape((3, 3))

        self._visualization = self.load_param("~visualization", True)
        self._result_pub = rospy.Publisher("~result", Result, queue_size=1)
        self._vis_pub = rospy.Publisher("~visualization", Image, queue_size=1)
        self._grasp_pub = rospy.Publisher("~grasp", Float32MultiArray, queue_size=1)
        self._depth_sub = rospy.Subscriber(
            "/camera/aligned_depth_to_color/image_meters",
            Image,
            self.callback_depth_image,
            queue_size=1,
        )
        self._depth_raw_sub = rospy.Subscriber(
            "/camera/aligned_depth_to_color/image_raw",
            Image,
            self.callback_depth_image_raw,
            queue_size=1,
        )
        self._image_sub = rospy.Subscriber(
            "/camera/color/image_raw", Image, self.callback_image, queue_size=1
        )
        self.start_time = time.time()
        rospy.logwarn("Initialized")

    def run(self):
        rate = rospy.Rate(100)
        while not rospy.is_shutdown():
            if (
                self._depth_msg_lock.acquire(False)
                and self._depth_raw_msg_lock.acquire(False)
                and self._image_msg_lock.acquire(False)
            ):
                depth_msg, depth_msg_raw, image_msg = (
                    self._last_depth_msg,
                    self._last_depth_raw_msg,
                    self._last_image_msg,
                )
                self._last_depth_msg, self._last_depth_raw_msg, self._last_image_msg = (
                    None,
                    None,
                    None,
                )
                self._depth_msg_lock.release()
                self._depth_raw_msg_lock.release()
                self._image_msg_lock.release()

            else:
                rate.sleep()
                continue

            if not depth_msg or not depth_msg_raw or not image_msg:
                rate.sleep()
                continue

            self._image_counter = self._image_counter + 1
            if (self._image_counter % 11) == 10:
                rospy.loginfo(
                    "Images detected per second=%.2f",
                    float(self._image_counter) / (time.time() - self.start_time),
                )
            np_image = self.convert_to_cv_image(image_msg)
            object_detection_results = self.object_detector(np_image)
            bounding_boxes = object_detection_results.xywh[0]
            bounding_boxes = [
                (t[2].item() * t[3].item(), t) for t in bounding_boxes
            ]  # sort by smallest area
            bounding_boxes.sort(key=lambda tup: tup[0])

            depth_img = self.convert_to_depth_img(depth_msg)
            depth_img = np.nan_to_num(depth_img, nan=np.inf)
            depth_img_raw = self.convert_to_depth_img(depth_msg_raw, raw=True)

            num_components, segmap = self.segmap_from_depthmap(depth_img_raw)
            sa_points_array = []
            for class_id in range(1, num_components):
                sa_points = self.smallest_axis(segmap, depth_img, class_id=class_id)
                if sa_points:
                    sa_points_array.append(sa_points)

            class_names = self.publish_grasp(
                sa_points_array,
                depth_img,
                bounding_boxes,
                object_detection_results.names,
            )

            # Visualize results
            if self._visualization:
                scale = 1
                img = Detectron2node.colormap_segmap(segmap)
                # img = np.moveaxis(img, 0, -1)

                # v = Visualizer(
                #     img[:, :, ::-1].T,
                #     MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]),
                #     scale=scale,
                # )

                # temp output
                # _temp_output = outputs["instances"].to("cpu") # removed
                # _class_ids = _temp_output.pred_classes if _temp_output.has("pred_classes") else None
                # _names = np.array(self._class_names)[_class_ids.numpy()]

                # for kkk, _name in enumerate(_names):
                #     if _name == 'toilet':
                #         print(_temp_output.pred_classes[_class_ids.numpy()[kkk]])
                #         _temp_output.pred_classes[_class_ids.numpy()[kkk]] = 'tofu'
                #     if _name == 'person':
                #         print(_temp_output.pred_classes[_class_ids.numpy()[kkk]])
                #         _temp_output.pred_classes[_class_ids.numpy()[kkk]] = 'tofu'

                # v = v.draw_instance_predictions()
                # img = v.get_image()[
                #     :, :, ::-1
                # ]
                img_contiguous = np.ascontiguousarray(img)
                # add grasp
                for class_name, sa_points in zip(class_names, sa_points_array):
                    cv.line(
                        img_contiguous,
                        (int(sa_points[1] * scale), int(sa_points[0] * scale)),
                        (int(sa_points[3] * scale), int(sa_points[2] * scale)),
                        (0, 0, 255),
                        5,
                    )

                    cv.circle(
                        img_contiguous,
                        (int(sa_points[5] * scale), int(sa_points[4] * scale)),
                        10,
                        (0, 255, 255),
                        -1,
                    )
                    cv.putText(img_contiguous, class_name, (int(sa_points[5]) + 30, int(sa_points[4])), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv.LINE_AA)

                image_msg = self._bridge.cv2_to_imgmsg(img_contiguous)
                self._vis_pub.publish(image_msg)

            rate.sleep()

    def segmap_from_depthmap_v1(self, depth_img):
        min_depth = np.min(depth_img)
        mask = depth_img < (min_depth + 0.02)  # +2cm
        print("frac ", np.argwhere(mask).shape[0] / (mask.shape[0] * mask.shape[1]))
        return mask.astype(int)  # convert bool to 0 and 1

    def segmap_from_depthmap(self, depth_img):
        blur = cv.GaussianBlur(depth_img, (7, 7), 0)
        blur = cv.blur(blur, (17, 17))
        # cut out top right corner which is noisy
        blur[:110, 675:] = 1000

        TABLE_DEPTH = 563
        ret, thres = cv.threshold(
            blur,
            TABLE_DEPTH,
            255,
            cv.THRESH_BINARY_INV,
        )
        # Marker labelling
        ret, markers = cv.connectedComponents(np.uint8(thres))
        print("num components", ret)
        return ret, np.uint8(markers)

    @staticmethod
    def draw_box_around_segmap(segmap):
        # it seems like the box is only for visualisation, so there is no need to find the tightest bounding box here
        coords = np.argwhere(segmap)
        min_y, min_x = np.min(np.argwhere(coords), axis=0)
        max_y, max_x = np.max(np.argwhere(coords), axis=0)
        return (min_x, min_y, max_x, max_y)

    @staticmethod
    def colormap_segmap(segmap):
        # map 0-numclasses to rgb
        segmap *= 25  # stretch values to make distinguishable
        return cv.applyColorMap(segmap, cv.COLORMAP_JET)

    def smallest_axis(self, segmap, depth_img, class_id=1):
        mask = np.ma.masked_where(segmap != class_id, segmap)  # filter out != class_id
        masked_depth_img = np.ma.masked_where(np.ma.getmask(mask), depth_img)
        top_coord = np.unravel_index(
            np.argmin(masked_depth_img), masked_depth_img.shape
        )

        top_surface_coords = set()
        queue = deque([top_coord])
        while queue:
            coord = queue.popleft()
            if coord in top_surface_coords:
                continue
            top_surface_coords.add(coord)
            for neighbor_d in ((1, 0), (-1, 0), (0, 1), (0, -1)):
                neighbor_coord = (coord[0] + neighbor_d[0], coord[1] + neighbor_d[1])
                if (
                    neighbor_coord[0] < 0  # out of bounds check
                    or neighbor_coord[0] >= masked_depth_img.shape[0]
                    or neighbor_coord[1] < 0
                    or neighbor_coord[1] >= masked_depth_img.shape[1]
                    or not bool(masked_depth_img[neighbor_coord])  # outside of mask
                    or abs(masked_depth_img[coord] - masked_depth_img[neighbor_coord])
                    > 0.01  # more than 1 cm gradient
                ):
                    continue
                queue.append(neighbor_coord)
        top_surface_coords = np.array(list(top_surface_coords))
        if len(top_surface_coords) == 0:
            return
        center = top_surface_coords.mean(axis=0)
        # run pca
        try:
            pca = PCA(n_components=2)
            pca.fit(top_surface_coords)
        except ValueError as e:
            print(e)
            return None

        # choose smallest component
        comp = pca.components_[1]
        var = pca.explained_variance_[1]
        comp = comp * var * 0.1
        x1 = center[0] - comp[0]
        y1 = center[1] - comp[1]

        x2 = center[0] + comp[0]
        y2 = center[1] + comp[1]

        # calculate angle
        angle = np.pi / 2 - np.arctan2(comp[1], comp[0])
        if angle >= np.pi:
            angle = angle - np.pi
        elif angle < -np.pi:
            angle = angle + np.pi

        return (
            x1,
            y1,
            x2,
            y2,
            center[0],
            center[1],
            angle,
        )  # center[0] = y, center[1] = x

    def publish_grasp(
        self, sa_points_array, depth_img, bounding_boxes, class_dict
    ) -> None:
        grasp_msg = Float32MultiArray()
        data = []
        class_names = []
        for sa_points in sa_points_array:
            object_center = (int(sa_points[5]), int(sa_points[4]))  # x, y
            class_name = Detectron2node.get_class_name_at_center(
                object_center, bounding_boxes, class_dict
            )
            class_names.append(class_name)
            depth = Detectron2node.get_depth_at_point(depth_img, object_center)
            if depth == 0 or depth == np.nan:
                continue
            x, y, z, r, _, _ = self.get_pose_at_img_coordinate(
                depth, object_center, sa_points[6]
            )
            if not x or not y:
                continue

            # make some corrections
            x -= 0.015
            y += 0.012
            z -= 0.24
            data.extend([x, y, z, r])  # TODO change message format
        grasp_msg.data = data
        self._grasp_pub.publish(grasp_msg)
        return class_names

    @staticmethod
    def get_class_name_at_center(object_center, bounding_boxes, class_ids):
        for _box_area, bb in bounding_boxes:
            x, y, dx, dy, class_id = (
                bb[0].item() - LEFT_X_OFFSET,
                bb[1].item(),
                bb[2].item() / 2,
                bb[3].item() / 2,
                bb[5].item(),
            )
            if (
                object_center[0] > x - dx
                and object_center[0] < x + dx
                and object_center[1] > y - dy
                and object_center[1] < y + dy
            ):  
                return class_ids[class_id]

    def convert_to_depth_img(self, depth_msg, raw=False):
        depth_img = np.ndarray(
            shape=(depth_msg.height, depth_msg.width),
            dtype=np.uint16 if raw else np.float32,
            buffer=depth_msg.data,
        )[:, LEFT_X_OFFSET : int(23 / 32 * depth_msg.width)]
        # cropping here to remove irrelevant sections
        return depth_img

    @staticmethod
    def get_depth_at_point(depth_img, object_center) -> float:
        return depth_img[object_center[1]][object_center[0]]

    def get_pose_at_img_coordinate(self, depth, coord: Tuple[int, int], angle: float):
        fx = self.cam_K[0, 0]
        fy = self.cam_K[1, 1]
        cx, cy = self.cam_K[0, 2], self.cam_K[1, 2]
        X = (LEFT_X_OFFSET + coord[0] - cx) / fx * depth
        Y = (coord[1] - cy) / fy * depth
        # print(X, Y, depth)
        return X, Y, depth, angle, 0, 0

    def convert_to_cv_image(self, image_msg):
        if image_msg is None:
            return None

        self._width = image_msg.width
        self._height = image_msg.height
        channels = int(len(image_msg.data) / (self._width * self._height))

        encoding = None
        if image_msg.encoding.lower() in ["rgb8", "bgr8"]:
            encoding = np.uint8
        elif image_msg.encoding.lower() == "mono8":
            encoding = np.uint8
        elif image_msg.encoding.lower() == "16uc1":
            encoding = np.uint16
            channels = 1
        elif image_msg.encoding.lower() == "32fc1":
            encoding = np.float32
            channels = 1

        cv_img = np.ndarray(
            shape=(image_msg.height, image_msg.width, channels),
            dtype=encoding,
            buffer=image_msg.data,
        )

        if image_msg.encoding.lower() == "mono8":
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2GRAY)
        elif image_msg.encoding.lower() == "16uc1":
            cv_img = cv_img.astype("int32")
        else:
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2BGR)

        return cv_img

    def callback_depth_image(self, msg):
        if self._depth_msg_lock.acquire(False):
            self._last_depth_msg = msg
            self._header = msg.header
            self._depth_msg_lock.release()

    def callback_depth_image_raw(self, msg):
        if self._depth_raw_msg_lock.acquire(False):
            self._last_depth_raw_msg = msg
            self._depth_raw_msg_lock.release()

    def callback_image(self, msg):
        if self._image_msg_lock.acquire(False):
            self._last_image_msg = msg
            self._image_msg_lock.release()

    @staticmethod
    def load_param(param, default=None):
        new_param = rospy.get_param(param, default)
        rospy.loginfo("[Detectron2] %s: %s", param, new_param)
        return new_param


def main(argv):
    rospy.init_node("detectron2_ros")
    node = Detectron2node()
    node.run()


if __name__ == "__main__":
    main(sys.argv)
