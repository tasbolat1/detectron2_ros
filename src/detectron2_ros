#!/usr/bin/env python
import sys
import threading
import time
from typing import Tuple
from math import pi

import cv2 as cv
import numpy as np
from scipy.spatial.transform import Rotation
from sklearn.decomposition import PCA
import rospy
from geometry_msgs.msg import PoseStamped
from sensor_msgs.msg import CameraInfo
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
from cv_bridge import CvBridge, CvBridgeError

# import some common detectron2 utilities
from detectron2.engine import DefaultPredictor
from detectron2.utils.logger import setup_logger
from detectron2.utils.visualizer import Visualizer
from detectron2_ros.msg import Result
from sensor_msgs.msg import Image, RegionOfInterest


class Detectron2node(object):
    def __init__(self):
        rospy.logwarn("Initializing")
        setup_logger()

        self._bridge = CvBridge()
        self._last_depth_msg = None
        self._depth_msg_lock = threading.Lock()
        self._image_counter = 0
        camera_info_msg = rospy.wait_for_message(
            "/camera/color/camera_info", CameraInfo
        )
        self.cam_K = np.array(camera_info_msg.K).reshape((3, 3))

        self.cfg = get_cfg()  # TODO find a way to remove this
        self.cfg.merge_from_file(self.load_param("~config"))
        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = self.load_param(
            "~detection_threshold"
        )  # set threshold for this model
        self.cfg.MODEL.WEIGHTS = self.load_param("~model")

        self._visualization = self.load_param("~visualization", True)
        self._result_pub = rospy.Publisher("~result", Result, queue_size=1)
        self._vis_pub = rospy.Publisher("~visualization", Image, queue_size=1)
        self._grasp_pub = rospy.Publisher("~grasp", PoseStamped, queue_size=1)
        # self._sub = rospy.Subscriber(
        #     self.load_param("~input"), Image, self.callback_image, queue_size=1
        # )
        self._depth_sub = rospy.Subscriber(
            "/camera/aligned_depth_to_color/image_meters",
            Image,
            self.callback_depth_image,
            queue_size=1,
        )
        self.start_time = time.time()
        rospy.logwarn("Initialized")

    def run(self):
        rate = rospy.Rate(100)
        while not rospy.is_shutdown():
            if self._depth_msg_lock.acquire(False):
                depth_msg = self._last_depth_msg
                self._last_depth_msg = None
                self._depth_msg_lock.release()

            else:
                rate.sleep()
                continue

            if depth_msg:
                self._image_counter = self._image_counter + 1
                if (self._image_counter % 11) == 10:
                    rospy.loginfo(
                        "Images detected per second=%.2f",
                        float(self._image_counter) / (time.time() - self.start_time),
                    )

                depth_img = Detectron2node.convert_to_depth_img(depth_msg)
                depth_img = np.nan_to_num(depth_img, nan=np.inf)

                segmap = self.segmap_from_depthmap(depth_img)
                sa_points = self.smallest_axis(segmap)
                print("Angle:", sa_points[-1] * 180 / 3.14)
                box = Detectron2node.draw_box_around_segmap(segmap)
                result_msg = self.get_result_msg([box], [segmap])

                self.publish_grasp(sa_points, depth_msg)
                self._result_pub.publish(result_msg)

                # Visualize results
                if self._visualization:
                    scale = 1
                    img = Detectron2node.colormap_depth(depth_img)
                    img = np.moveaxis(img, 0, -1)
                    img = img.astype(np.uint8)
                    # v = Visualizer(
                    #     img[:, :, ::-1].T,
                    #     MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]),
                    #     scale=scale,
                    # )

                    # temp output
                    # _temp_output = outputs["instances"].to("cpu") # removed
                    # _class_ids = _temp_output.pred_classes if _temp_output.has("pred_classes") else None
                    # _names = np.array(self._class_names)[_class_ids.numpy()]

                    # for kkk, _name in enumerate(_names):
                    #     if _name == 'toilet':
                    #         print(_temp_output.pred_classes[_class_ids.numpy()[kkk]])
                    #         _temp_output.pred_classes[_class_ids.numpy()[kkk]] = 'tofu'
                    #     if _name == 'person':
                    #         print(_temp_output.pred_classes[_class_ids.numpy()[kkk]])
                    #         _temp_output.pred_classes[_class_ids.numpy()[kkk]] = 'tofu'

                    # v = v.draw_instance_predictions()
                    # img = v.get_image()[
                    #     :, :, ::-1
                    # ]
                    img_contiguous = np.ascontiguousarray(img)
                    # add grasp
                    if sa_points:
                        cv.line(
                            img_contiguous,
                            (int(sa_points[1] * scale), int(sa_points[0] * scale)),
                            (int(sa_points[3] * scale), int(sa_points[2] * scale)),
                            (0, 0, 255),
                            5,
                        )

                        cv.circle(
                            img_contiguous,
                            (int(sa_points[5] * scale), int(sa_points[4] * scale)),
                            10,
                            (0, 255, 255),
                            -1,
                        )

                    image_msg = self._bridge.cv2_to_imgmsg(img_contiguous)
                    self._vis_pub.publish(image_msg)

            rate.sleep()

    def segmap_from_depthmap(self, depth_img):
        min_depth = np.min(depth_img)
        mask = depth_img < (min_depth + 0.02)  # +2cm
        print("frac ", np.argwhere(mask).shape[0] / (mask.shape[0] * mask.shape[1]))
        return mask.astype(int)  # convert bool to 0 and 1

    @staticmethod
    def draw_box_around_segmap(segmap):
        # it seems like the box is only for visualisation, so there is no need to find the tightest bounding box here
        coords = np.argwhere(segmap)
        min_y, min_x = np.min(np.argwhere(coords), axis=0)
        max_y, max_x = np.max(np.argwhere(coords), axis=0)
        return (min_x, min_y, max_x, max_y)

    @staticmethod
    def colormap_depth(depth_img):
        # map 0-1.5m to rgb
        single_channel = np.interp(depth_img, [0.1, 1.5], [255, 0])
        return np.array([single_channel, single_channel, single_channel])

    def smallest_axis(self, mask):
        mask_xy = np.argwhere(mask != 0)
        center = mask_xy.mean(axis=0)

        # run pca
        pca = PCA(n_components=2)
        pca.fit(mask_xy)
        # choose smallest component
        comp = pca.components_[1]
        var = pca.explained_variance_[1]
        comp = comp * var * 0.1
        x1 = center[0] - comp[0]
        y1 = center[1] - comp[1]

        x2 = comp[0] + center[0]
        y2 = comp[1] + center[1]

        # calculate angle
        angle = np.arctan2(comp[1], comp[0]) + np.pi / 2
        if angle >= np.pi:
            angle = angle - np.pi
        elif angle <= -np.pi:
            angle = angle + np.pi

        return (
            x1,
            y1,
            x2,
            y2,
            center[0],
            center[1],
            angle,
        )  # center[0] = y, center[1] = x

    def get_result_msg(self, boxes, masks):
        result_msg = Result()
        result_msg.header = self._header

        # # replace toilet to tofu
        # _temp = np.array(self._class_names)[result_msg.class_ids.numpy()]
        # for i, nnn in enumerate(result_msg.class_names):
        #     if nnn == 'toilet':
        #         result_msg.class_names[i] = 'tofu'
        #     elif nnn == 'person':
        #         result_msg.class_names[i] = 'tofu'

        for i, (x1, y1, x2, y2) in enumerate(boxes):
            mask = np.zeros(masks[i].shape, dtype="uint8")
            mask[masks[i][:, :]] = 255
            mask = self._bridge.cv2_to_imgmsg(mask)
            result_msg.masks.append(mask)

            box = RegionOfInterest()
            box.x_offset = np.uint32(x1)
            box.y_offset = np.uint32(y1)
            box.height = np.uint32(y2 - y1)
            box.width = np.uint32(x2 - x1)
            result_msg.boxes.append(box)

        return result_msg

    def publish_grasp(self, sa_points, depth_msg) -> None:
        if not sa_points:
            return
        depth_img = Detectron2node.convert_to_depth_img(depth_msg)
        object_center = (int(sa_points[5]), int(sa_points[4]))  # x, y
        depth = Detectron2node.get_depth_at_point(depth_img, object_center)
        if depth == 0 or depth == np.nan:
            return
        x, y, z, r, _, _ = self.get_pose_at_img_coordinate(
            depth, object_center, sa_points[6]
        )
        if not x or not y:
            return
        if r > pi / 2:  # make angle acute
            r = -(pi - r)
        elif r < -pi / 4:
            r = pi + r
        r += pi / 2

        grasp_msg = PoseStamped()
        grasp_msg.header.stamp = rospy.Time.now()
        grasp_msg.header.frame_id = "camera_color_optical_frame"

        grasp_msg.pose.position.x = x - 0.015
        grasp_msg.pose.position.y = y + 0.012
        grasp_msg.pose.position.z = z - 0.23
        ow, ox, oy, oz = quaternion_from_euler(r, 0, 0)
        grasp_msg.pose.orientation.w = ow
        grasp_msg.pose.orientation.x = ox
        grasp_msg.pose.orientation.y = oy
        grasp_msg.pose.orientation.z = oz

        self._grasp_pub.publish(grasp_msg)

    @staticmethod
    def convert_to_depth_img(depth_msg):
        return np.ndarray(
            shape=(depth_msg.height, depth_msg.width),
            dtype=np.float32,
            buffer=depth_msg.data,
        )

    @staticmethod
    def get_depth_at_point(depth_img, object_center) -> float:
        return depth_img[object_center[1]][object_center[0]]

    def get_pose_at_img_coordinate(self, depth, coord: Tuple[int, int], angle: float):
        fx = self.cam_K[0, 0]
        fy = self.cam_K[1, 1]
        cx, cy = self.cam_K[0, 2], self.cam_K[1, 2]
        X = (coord[0] - cx) / fx * depth
        Y = (coord[1] - cy) / fy * depth
        # print(X, Y, depth)
        return X, Y, depth, angle, 0, 0

    def convert_to_cv_image(self, image_msg):
        if image_msg is None:
            return None

        self._width = image_msg.width
        self._height = image_msg.height
        channels = int(len(image_msg.data) / (self._width * self._height))

        encoding = None
        if image_msg.encoding.lower() in ["rgb8", "bgr8"]:
            encoding = np.uint8
        elif image_msg.encoding.lower() == "mono8":
            encoding = np.uint8
        elif image_msg.encoding.lower() == "16uc1":
            encoding = np.uint16
            channels = 1
        elif image_msg.encoding.lower() == "32fc1":
            encoding = np.float32
            channels = 1

        cv_img = np.ndarray(
            shape=(image_msg.height, image_msg.width, channels),
            dtype=encoding,
            buffer=image_msg.data,
        )

        if image_msg.encoding.lower() == "mono8":
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2GRAY)
        elif image_msg.encoding.lower() == "16uc1":
            cv_img = cv_img.astype("int32")
        else:
            cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2BGR)

        return cv_img

    def callback_depth_image(self, msg):
        rospy.logdebug("Get a depth image")
        if self._depth_msg_lock.acquire(False):
            self._last_depth_msg = msg
            self._header = msg.header
            self._depth_msg_lock.release()

    @staticmethod
    def load_param(param, default=None):
        new_param = rospy.get_param(param, default)
        rospy.loginfo("[Detectron2] %s: %s", param, new_param)
        return new_param


def quaternion_from_euler(r, p, y):
    rot = Rotation.from_euler("xyz", [r, p, y])
    return rot.as_quat()


def main(argv):
    rospy.init_node("detectron2_ros")
    node = Detectron2node()
    node.run()


if __name__ == "__main__":
    main(sys.argv)
